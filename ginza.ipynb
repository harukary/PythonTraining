{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GiNZA\r\n",
    "* 形態素解析\r\n",
    "* 係り受け関係解析"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"ja_ginza\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# パイプライン構成\r\n",
    "for p in nlp.pipeline:\r\n",
    "    print(p)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('tagger', <spacy.pipeline.pipes.Tagger object at 0x000002CD95E590D0>)\n",
      "('parser', <spacy.pipeline.pipes.DependencyParser object at 0x000002CD961D7D60>)\n",
      "('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x000002CD961D7D00>)\n",
      "('CompoundSplitter', <ginza.compound_splitter.CompoundSplitter object at 0x000002CD9256E280>)\n",
      "('BunsetuRecognizer', <ginza.bunsetu_recognizer.BunsetuRecognizer object at 0x000002CD92369B20>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "doc = nlp(\"ひき肉に、炒めて冷ましたたまねぎ、パン粉と牛乳と卵を混ぜる。\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# 形態素解析結果\r\n",
    "for sent in doc.sents:\r\n",
    "    for token in sent:\r\n",
    "        print(token.i, token.orth_, token.lemma_, token.pos_, token.dep_, token.head.i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 ひき肉 ひき肉 NOUN obl 15\n",
      "1 に に ADP case 0\n",
      "2 、 、 PUNCT punct 0\n",
      "3 炒め 炒める VERB advcl 5\n",
      "4 て て SCONJ mark 3\n",
      "5 冷まし 冷ます VERB acl 7\n",
      "6 た た AUX aux 5\n",
      "7 たまねぎ たまねぎ NOUN obl 15\n",
      "8 、 、 PUNCT punct 7\n",
      "9 パン粉 パン粉 NOUN nmod 11\n",
      "10 と と ADP case 9\n",
      "11 牛乳 牛乳 NOUN nmod 13\n",
      "12 と と ADP case 11\n",
      "13 卵 卵 NOUN obj 15\n",
      "14 を を ADP case 13\n",
      "15 混ぜる 混ぜる VERB ROOT 15\n",
      "16 。 。 PUNCT punct 15\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Token\r\n",
    "print('token-----------------------')\r\n",
    "for token in doc:\r\n",
    "  print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "token-----------------------\n",
      "ひき肉\n",
      "に\n",
      "、\n",
      "炒め\n",
      "て\n",
      "冷まし\n",
      "た\n",
      "たまねぎ\n",
      "、\n",
      "パン粉\n",
      "と\n",
      "牛乳\n",
      "と\n",
      "卵\n",
      "を\n",
      "混ぜる\n",
      "。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# 名詞句のみ抽出\r\n",
    "print('noun-----------------------')\r\n",
    "for np in doc.noun_chunks:\r\n",
    "  print(np)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "noun-----------------------\n",
      "ひき肉\n",
      "炒めて冷ましたたまねぎ\n",
      "パン粉\n",
      "牛乳\n",
      "卵\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# 固有表現\r\n",
    "print('ner-----------------------')\r\n",
    "for ent in doc.ents:\r\n",
    "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ner-----------------------\n",
      "たまねぎ 12 16 Food_Other\n",
      "パン粉 17 20 Food_Other\n",
      "牛乳 21 23 Food_Other\n",
      "卵 24 25 Food_Other\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "print(help(doc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on Doc object:\n",
      "\n",
      "class Doc(builtins.object)\n",
      " |  A sequence of Token objects. Access sentences and named entities, export\n",
      " |  annotations to numpy arrays, losslessly serialize to compressed binary\n",
      " |  strings. The `Doc` object holds an array of `TokenC` structs. The\n",
      " |  Python-level `Token` and `Span` objects are views of this array, i.e.\n",
      " |  they don't own the data themselves.\n",
      " |  \n",
      " |  EXAMPLE:\n",
      " |      Construction 1\n",
      " |      >>> doc = nlp(u'Some text')\n",
      " |  \n",
      " |      Construction 2\n",
      " |      >>> from spacy.tokens import Doc\n",
      " |      >>> doc = Doc(nlp.vocab, words=[u'hello', u'world', u'!'],\n",
      " |      >>>           spaces=[True, False, False])\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/doc\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      Get a `Token` or `Span` object.\n",
      " |      \n",
      " |      i (int or tuple) The index of the token, or the slice of the document\n",
      " |          to get.\n",
      " |      RETURNS (Token or Span): The token at `doc[i]]`, or the span at\n",
      " |          `doc[start : end]`.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc[i]\n",
      " |          Get the `Token` object at position `i`, where `i` is an integer.\n",
      " |          Negative indexing is supported, and follows the usual Python\n",
      " |          semantics, i.e. `doc[-2]` is `doc[len(doc) - 2]`.\n",
      " |      \n",
      " |          >>> doc[start : end]]\n",
      " |          Get a `Span` object, starting at position `start` and ending at\n",
      " |          position `end`, where `start` and `end` are token indices. For\n",
      " |          instance, `doc[2:5]` produces a span consisting of tokens 2, 3 and\n",
      " |          4. Stepped slices (e.g. `doc[start : end : step]`) are not\n",
      " |          supported, as `Span` objects must be contiguous (cannot have gaps).\n",
      " |          You can use negative indices and open-ended ranges, which have\n",
      " |          their normal Python semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#getitem\n",
      " |  \n",
      " |  __init__(...)\n",
      " |      Create a Doc object.\n",
      " |      \n",
      " |      vocab (Vocab): A vocabulary object, which must match any models you\n",
      " |          want to use (e.g. tokenizer, parser, entity recognizer).\n",
      " |      words (list or None): A list of unicode strings to add to the document\n",
      " |          as words. If `None`, defaults to empty list.\n",
      " |      spaces (list or None): A list of boolean values, of the same length as\n",
      " |          words. True means that the word is followed by a space, False means\n",
      " |          it is not. If `None`, defaults to `[True]*len(words)`\n",
      " |      user_data (dict or None): Optional extra data to attach to the Doc.\n",
      " |      RETURNS (Doc): The newly constructed object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#init\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      Iterate over `Token`  objects, from which the annotations can be\n",
      " |      easily accessed. This is the main way of accessing `Token` objects,\n",
      " |      which are the main way annotations are accessed from Python. If faster-\n",
      " |      than-Python speeds are required, you can instead access the annotations\n",
      " |      as a numpy array, or access the underlying C data directly from Cython.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#iter\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of tokens in the document.\n",
      " |      \n",
      " |      RETURNS (int): The number of tokens in the document.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#len\n",
      " |  \n",
      " |  __reduce__ = __reduce_cython__(...)\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__ = __setstate_cython__(...)\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |  \n",
      " |  char_span(...)\n",
      " |      Create a `Span` object from the slice\n",
      " |      `doc.text[start_idx : end_idx]`. Returns None if no valid `Span` can be\n",
      " |      created.\n",
      " |      \n",
      " |      doc (Doc): The parent document.\n",
      " |      start_idx (int): The index of the first character of the span.\n",
      " |      end_idx (int): The index of the first character after the span.\n",
      " |      label (uint64 or string): A label to attach to the Span, e.g. for\n",
      " |          named entities.\n",
      " |      kb_id (uint64 or string):  An ID from a KB to capture the meaning of a\n",
      " |          named entity.\n",
      " |      vector (ndarray[ndim=1, dtype='float32']): A meaning representation of\n",
      " |          the span.\n",
      " |      alignment_mode (str): How character indices are aligned to token\n",
      " |          boundaries. Options: \"strict\" (character indices must be aligned\n",
      " |          with token boundaries), \"contract\" (span of all tokens completely\n",
      " |          within the character span), \"expand\" (span of all tokens at least\n",
      " |          partially covered by the character span). Defaults to \"strict\".\n",
      " |      RETURNS (Span): The newly constructed object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#char_span\n",
      " |  \n",
      " |  count_by(...)\n",
      " |      Count the frequencies of a given attribute. Produces a dict of\n",
      " |      `{attribute (int): count (ints)}` frequencies, keyed by the values of\n",
      " |      the given attribute ID.\n",
      " |      \n",
      " |      attr_id (int): The attribute ID to key the counts.\n",
      " |      RETURNS (dict): A dictionary mapping attributes to integer counts.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#count_by\n",
      " |  \n",
      " |  extend_tensor(...)\n",
      " |      Concatenate a new tensor onto the doc.tensor object.\n",
      " |      \n",
      " |      The doc.tensor attribute holds dense feature vectors\n",
      " |      computed by the models in the pipeline. Let's say a\n",
      " |      document with 30 words has a tensor with 128 dimensions\n",
      " |      per word. doc.tensor.shape will be (30, 128). After\n",
      " |      calling doc.extend_tensor with an array of shape (30, 64),\n",
      " |      doc.tensor == (30, 192).\n",
      " |  \n",
      " |  from_array(...)\n",
      " |      Load attributes from a numpy array. Write to a `Doc` object, from an\n",
      " |      `(M, N)` array of attributes.\n",
      " |      \n",
      " |      attrs (list) A list of attribute ID ints.\n",
      " |      array (numpy.ndarray[ndim=2, dtype='int32']): The attribute values.\n",
      " |      RETURNS (Doc): Itself.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#from_array\n",
      " |  \n",
      " |  from_bytes(...)\n",
      " |      Deserialize, i.e. import the document contents from a binary string.\n",
      " |      \n",
      " |      data (bytes): The string to load from.\n",
      " |      exclude (list): String names of serialization fields to exclude.\n",
      " |      RETURNS (Doc): Itself.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#from_bytes\n",
      " |  \n",
      " |  from_disk(...)\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it.\n",
      " |      \n",
      " |      path (unicode or Path): A path to a directory. Paths may be either\n",
      " |          strings or `Path`-like objects.\n",
      " |      exclude (list): String names of serialization fields to exclude.\n",
      " |      RETURNS (Doc): The modified `Doc` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#from_disk\n",
      " |  \n",
      " |  get_lca_matrix(...)\n",
      " |      Calculates a matrix of Lowest Common Ancestors (LCA) for a given\n",
      " |      `Doc`, where LCA[i, j] is the index of the lowest common ancestor among\n",
      " |      token i and j.\n",
      " |      \n",
      " |      RETURNS (np.array[ndim=2, dtype=numpy.int32]): LCA matrix with shape\n",
      " |          (n, n), where n = len(self).\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#get_lca_matrix\n",
      " |  \n",
      " |  merge(...)\n",
      " |      Retokenize the document, such that the span at\n",
      " |      `doc.text[start_idx : end_idx]` is merged into a single token. If\n",
      " |      `start_idx` and `end_idx `do not mark start and end token boundaries,\n",
      " |      the document remains unchanged.\n",
      " |      \n",
      " |      start_idx (int): Character index of the start of the slice to merge.\n",
      " |      end_idx (int): Character index after the end of the slice to merge.\n",
      " |      **attributes: Attributes to assign to the merged token. By default,\n",
      " |          attributes are inherited from the syntactic root of the span.\n",
      " |      RETURNS (Token): The newly merged token, or `None` if the start and end\n",
      " |          indices did not fall at token boundaries.\n",
      " |  \n",
      " |  print_tree(...)\n",
      " |  \n",
      " |  retokenize(...)\n",
      " |      Context manager to handle retokenization of the Doc.\n",
      " |      Modifications to the Doc's tokenization are stored, and then\n",
      " |      made all at once when the context manager exits. This is\n",
      " |      much more efficient, and less error-prone.\n",
      " |      \n",
      " |      All views of the Doc (Span and Token) created before the\n",
      " |      retokenization are invalidated, although they may accidentally\n",
      " |      continue to work.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#retokenize\n",
      " |      USAGE: https://spacy.io/usage/linguistic-features#retokenization\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |      similarity using an average of word vectors.\n",
      " |      \n",
      " |      other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |          `Span`, `Token` and `Lexeme` objects.\n",
      " |      RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#similarity\n",
      " |  \n",
      " |  to_array(...)\n",
      " |      Export given token attributes to a numpy `ndarray`.\n",
      " |      If `attr_ids` is a sequence of M attributes, the output array will be\n",
      " |      of shape `(N, M)`, where N is the length of the `Doc` (in tokens). If\n",
      " |      `attr_ids` is a single attribute, the output shape will be (N,). You\n",
      " |      can specify attributes by integer ID (e.g. spacy.attrs.LEMMA) or\n",
      " |      string name (e.g. 'LEMMA' or 'lemma').\n",
      " |      \n",
      " |      attr_ids (list[]): A list of attributes (int IDs or string names).\n",
      " |      RETURNS (numpy.ndarray[long, ndim=2]): A feature matrix, with one row\n",
      " |          per word, and one column per attribute indicated in the input\n",
      " |          `attr_ids`.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
      " |          >>> doc = nlp(text)\n",
      " |          >>> # All strings mapped to integers, for easy export to numpy\n",
      " |          >>> np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
      " |  \n",
      " |  to_bytes(...)\n",
      " |      Serialize, i.e. export the document contents to a binary string.\n",
      " |      \n",
      " |      exclude (list): String names of serialization fields to exclude.\n",
      " |      RETURNS (bytes): A losslessly serialized copy of the `Doc`, including\n",
      " |          all annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#to_bytes\n",
      " |  \n",
      " |  to_disk(...)\n",
      " |      Save the current state to a directory.\n",
      " |      \n",
      " |      path (unicode or Path): A path to a directory, which will be created if\n",
      " |          it doesn't exist. Paths may be either strings or Path-like objects.\n",
      " |      exclude (list): String names of serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#to_disk\n",
      " |  \n",
      " |  to_json(...)\n",
      " |      Convert a Doc to JSON. The format it produces will be the new format\n",
      " |      for the `spacy train` command (not implemented yet).\n",
      " |      \n",
      " |      underscore (list): Optional list of string names of custom doc._.\n",
      " |      attributes. Attribute values need to be JSON-serializable. Values will\n",
      " |      be added to an \"_\" key in the data, e.g. \"_\": {\"foo\": \"bar\"}.\n",
      " |      RETURNS (dict): The data in spaCy's JSON format.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#to_json\n",
      " |  \n",
      " |  to_utf8_array(...)\n",
      " |      Encode word strings to utf8, and export to a fixed-width array\n",
      " |      of characters. Characters are placed into the array in the order:\n",
      " |          0, -1, 1, -2, etc\n",
      " |      For example, if the array is sliced array[:, :8], the array will\n",
      " |      contain the first 4 characters and last 4 characters of each word ---\n",
      " |      with the middle characters clipped out. The value 255 is used as a pad\n",
      " |      value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |      Look up a previously registered extension by name.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (tuple): A `(default, method, getter, setter)` tuple.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#get_extension\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |      Check whether an extension has been registered.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (bool): Whether the extension has been registered.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#has_extension\n",
      " |  \n",
      " |  remove_extension(...) from builtins.type\n",
      " |      Remove a previously registered extension.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (tuple): A `(default, method, getter, setter)` tuple of the\n",
      " |          removed extension.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#remove_extension\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |      Define a custom attribute which becomes available as `Doc._`.\n",
      " |      \n",
      " |      name (unicode): Name of the attribute to set.\n",
      " |      default: Optional default value of the attribute.\n",
      " |      getter (callable): Optional getter function.\n",
      " |      setter (callable): Optional setter function.\n",
      " |      method (callable): Optional method for method extension.\n",
      " |      force (bool): Force overwriting existing attribute.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#set_extension\n",
      " |      USAGE: https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cats\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ents\n",
      " |      The named entities in the document. Returns a tuple of named entity\n",
      " |      `Span` objects, if the entity recognizer has been applied.\n",
      " |      \n",
      " |      RETURNS (tuple): Entities in the document, one `Span` per entity.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#ents\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#has_vector\n",
      " |  \n",
      " |  is_nered\n",
      " |      Check if the document has named entities set. Will return True if\n",
      " |      *any* of the tokens has a named entity tag set (even if the others are\n",
      " |      unknown values), or if the document is empty.\n",
      " |  \n",
      " |  is_parsed\n",
      " |  \n",
      " |  is_sentenced\n",
      " |      Check if the document has sentence boundaries assigned. This is\n",
      " |      defined as having at least one of the following:\n",
      " |      \n",
      " |      a) An entry \"sents\" in doc.user_hooks\";\n",
      " |      b) Doc.is_parsed is set to True;\n",
      " |      c) At least one token other than the first where sent_start is not None.\n",
      " |  \n",
      " |  is_tagged\n",
      " |  \n",
      " |  lang\n",
      " |      RETURNS (uint64): ID of the language of the doc's vocabulary.\n",
      " |  \n",
      " |  lang_\n",
      " |      RETURNS (unicode): Language of the doc's vocabulary, e.g. 'en'.\n",
      " |  \n",
      " |  mem\n",
      " |  \n",
      " |  noun_chunks\n",
      " |      Iterate over the base noun phrases in the document. Yields base\n",
      " |      noun-phrase #[code Span] objects, if the document has been\n",
      " |      syntactically parsed. A base noun phrase, or \"NP chunk\", is a noun\n",
      " |      phrase that does not permit other NPs to be nested within it – so no\n",
      " |      NP-level coordination, no prepositional phrases, and no relative\n",
      " |      clauses.\n",
      " |      \n",
      " |      YIELDS (Span): Noun chunks in the document.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#noun_chunks\n",
      " |  \n",
      " |  noun_chunks_iterator\n",
      " |  \n",
      " |  sentiment\n",
      " |  \n",
      " |  sents\n",
      " |      Iterate over the sentences in the document. Yields sentence `Span`\n",
      " |      objects. Sentence spans have no label. To improve accuracy on informal\n",
      " |      texts, spaCy calculates sentence boundaries from the syntactic\n",
      " |      dependency parse. If the parser is disabled, the `sents` iterator will\n",
      " |      be unavailable.\n",
      " |      \n",
      " |      YIELDS (Span): Sentences in the document.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#sents\n",
      " |  \n",
      " |  tensor\n",
      " |  \n",
      " |  text\n",
      " |      A unicode representation of the document text.\n",
      " |      \n",
      " |      RETURNS (unicode): The original verbatim text of the document.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      An alias of `Doc.text`, provided for duck-type compatibility with\n",
      " |      `Span` and `Token`.\n",
      " |      \n",
      " |      RETURNS (unicode): The original verbatim text of the document.\n",
      " |  \n",
      " |  user_data\n",
      " |  \n",
      " |  user_hooks\n",
      " |  \n",
      " |  user_span_hooks\n",
      " |  \n",
      " |  user_token_hooks\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation. Defaults to an average of the\n",
      " |      token vectors.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the document's semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#vector\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the document's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#vector_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "抽出できる固有表現の種類ですが、上記の結果から推察するに、こちら*6だと思います。とは言っても GiNZA がどのようなデータで学習したか確認できてないので、はっきりしたことは言えません。ただ、どのような固有表現を抽出したいかは、要件次第のところもあります。GiNZA の git リポジトリに含まれる学習スクリプト*7を見ると、こちら*8 の train_data の形式で学習データを食わせてやれば良さそうなので、興味のある人は頑張ってみてください。spaCy のドキュメント*9によれば少なくとも数百件は必要とのことです。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# 係り受け関係表示\r\n",
    "from nltk import Tree\r\n",
    "\r\n",
    "# tree表示するトークンのフォーマット\r\n",
    "def token_format(tk):\r\n",
    "    # トークンのorth_、dep_、pos_の３つをトークンの情報として含める\r\n",
    "    return \"_\".join([tk.orth_, tk.dep_, tk.pos_])\r\n",
    "\r\n",
    "# tree表示する関数\r\n",
    "def to_nltk_tree(node):\r\n",
    "    if node.n_lefts + node.n_rights > 0:\r\n",
    "        return Tree(token_format(node), [to_nltk_tree(child) for child in node.children])\r\n",
    "    else:\r\n",
    "        return token_format(node)\r\n",
    "\r\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                                  混ぜる_ROOT_VERB                                                                            \n",
      "       _________________________________________________________________|_________________________________________________                                  \n",
      "      |                       |                                   たまねぎ_obl_NOUN                                       卵_obj_NOUN                           \n",
      "      |                       |                            _____________|____________                           __________|___________                      \n",
      "      |                       |                           |                     冷まし_acl_VERB                   |                 牛乳_nmod_NOUN              \n",
      "      |                       |                           |              ____________|_____________            |           ___________|_____________        \n",
      "      |                  ひき肉_obl_NOUN                     |             |                    炒め_advcl_VERB     |          |                   パン粉_nmod_NOUN\n",
      "      |            ___________|_____________              |             |                          |           |          |                         |       \n",
      "。_punct_PUNCT に_case_ADP              、_punct_PUNCT 、_punct_PUNCT   た_aux_AUX                 て_mark_SCONJ を_case_ADP と_case_ADP                と_case_ADP \n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# 係り受け関係解析：グラフ表示\r\n",
    "from spacy import displacy\r\n",
    "\r\n",
    "for sent in doc.sents:\r\n",
    "    svg = displacy.render(sent, style=\"dep\", options={\"compact\":True})"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"ja\" id=\"ee1e84eb02ab4d6895072ff62d7ac1c5-0\" class=\"displacy\" width=\"2150\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ひき肉</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">に、</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">炒め</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">て</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">冷まし</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">た</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">たまねぎ、</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">パン粉</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">と</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">牛乳</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">と</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1700\">卵</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1700\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">を</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2000\">混ぜる。</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2000\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-0\" stroke-width=\"2px\" d=\"M62,302.0 62,202.0 2000.0,202.0 2000.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,304.0 L58,296.0 66,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-1\" stroke-width=\"2px\" d=\"M62,302.0 62,277.0 191.0,277.0 191.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M191.0,304.0 L195.0,296.0 187.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-2\" stroke-width=\"2px\" d=\"M362,302.0 362,252.0 644.0,252.0 644.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,304.0 L358,296.0 366,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-3\" stroke-width=\"2px\" d=\"M362,302.0 362,277.0 491.0,277.0 491.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M491.0,304.0 L495.0,296.0 487.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-4\" stroke-width=\"2px\" d=\"M662,302.0 662,252.0 944.0,252.0 944.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,304.0 L658,296.0 666,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-5\" stroke-width=\"2px\" d=\"M662,302.0 662,277.0 791.0,277.0 791.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M791.0,304.0 L795.0,296.0 787.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-6\" stroke-width=\"2px\" d=\"M962,302.0 962,227.0 1997.0,227.0 1997.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M962,304.0 L958,296.0 966,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-7\" stroke-width=\"2px\" d=\"M1112,302.0 1112,252.0 1394.0,252.0 1394.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1112,304.0 L1108,296.0 1116,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-8\" stroke-width=\"2px\" d=\"M1112,302.0 1112,277.0 1241.0,277.0 1241.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1241.0,304.0 L1245.0,296.0 1237.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-9\" stroke-width=\"2px\" d=\"M1412,302.0 1412,252.0 1694.0,252.0 1694.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1412,304.0 L1408,296.0 1416,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-10\" stroke-width=\"2px\" d=\"M1412,302.0 1412,277.0 1541.0,277.0 1541.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1541.0,304.0 L1545.0,296.0 1537.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-11\" stroke-width=\"2px\" d=\"M1712,302.0 1712,252.0 1994.0,252.0 1994.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1712,304.0 L1708,296.0 1716,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-12\" stroke-width=\"2px\" d=\"M1712,302.0 1712,277.0 1841.0,277.0 1841.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ee1e84eb02ab4d6895072ff62d7ac1c5-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1841.0,304.0 L1845.0,296.0 1837.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# 係り受け関係解析：料理手順\r\n",
    "\r\n",
    "nlp = spacy.load('ja_ginza')\r\n",
    "\r\n",
    "instruction_list = [\r\n",
    "  \"ひき肉に、炒めて冷めたたまねぎ、パン粉と牛乳と卵を混ぜる。塩、胡椒、カレー粉を加えて、さらによく練る。小判上に成形して裏表にパン粉をつける。真ん中をくぼます。\",\r\n",
    "  \"［ポーチドエッグ］水、酢、塩を入れて沸騰させ、割った卵を鍋の縁からゆっくり入れる。卵が茹で上がったら水に取り出す。\",\r\n",
    "  \"赤ワインを火に掛けアルコールを飛ばし、水、トマトケチャップ、トマトピューレを合わせ、ソースを作る。\",\r\n",
    "  \"熱したフライパンにサラダ油を入れ、中火でハンバーグのパテを入れる。1分程度焼く、裏返し、裏も1分程度焼く。ハンバーグを取り出し、フライパンの油を捨てる。\",\r\n",
    "  \"中火にし、玉葱の輪切りを入れる。玉葱が焼けたら裏返し、ハンバーグを上に置く。のせたハンバーグにソースの材料を加え、フライパンにフタをし2～3分蒸し焼きにする。\",\r\n",
    "  \"竹串で突き透明の肉汁が出たら取り出す。肉汁を煮立て、塩胡椒を加えソースを完成させる。ハンバーグを皿に盛り、玉葱の輪切りとポーチドエッグをのせ、ソースをかける。\"\r\n",
    "]\r\n",
    "\r\n",
    "# instruction = ''.join(instruction_list)\r\n",
    "docs = list(nlp.pipe(instruction_list))\r\n",
    "\r\n",
    "for doc in docs:\r\n",
    "  for ent in doc.ents:\r\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\r\n",
    "\r\n",
    "# for sent in doc.sents:\r\n",
    "#     svg = displacy.render(sent, style=\"dep\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "たまねぎ 11 15 Food_Other\n",
      "パン粉 16 19 Food_Other\n",
      "牛乳 20 22 Food_Other\n",
      "卵 23 24 Food_Other\n",
      "塩 29 30 Food_Other\n",
      "胡椒 31 33 Food_Other\n",
      "カレー粉 34 38 Dish\n",
      "パン粉 62 65 Food_Other\n",
      "ポーチドエッグ 1 8 Doctrine_Method_Other\n",
      "酢 11 12 Food_Other\n",
      "塩 13 14 Food_Other\n",
      "卵 26 27 Food_Other\n",
      "卵 41 42 Food_Other\n",
      "赤ワイン 0 4 Dish\n",
      "ソース 42 45 Food_Other\n",
      "ハンバーグ 20 25 Dish\n",
      "1分程度 33 37 Period_Time\n",
      "1分程度 46 50 Period_Time\n",
      "ハンバーグ 53 58 Dish\n",
      "油 70 71 Food_Other\n",
      "玉葱 5 7 Food_Other\n",
      "玉葱 16 18 Flora_Part\n",
      "ハンバーグ 27 32 Dish\n",
      "ハンバーグ 41 46 Dish\n",
      "2～3分 67 71 Period_Time\n",
      "肉汁 8 10 Animal_Part\n",
      "肉汁 19 21 Animal_Part\n",
      "ソース 32 35 Food_Other\n",
      "ハンバーグ 42 47 Dish\n",
      "玉葱 53 55 Food_Other\n",
      "ソース 71 74 Food_Other\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "703bbbeedfcbeab203e322e73325805c9ac8ffb4b5321a34c42f421369651d04"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}